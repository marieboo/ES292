# Chapter 1: Golem of Prague

## 1.1. Statistical golem 
-- statistical models, models in general are programmed procedures (no thinking)

-- scientist as creators of models must select appropriate for context, otherwise they fail

-- standard statistical tests are fine, as long as we stay within well-tested context
e.g. linear regression, very specialized tests...


- BUT: classical tools are not diverse enough to handle common research questions

- Need: Rethinking 
"Statistical inference as set of strategies and not set of pre-made tools"

## 1.2. Statistical rethinking

Problems:

-- Choosing wrong tests

-- not understanding how tests work

-- concept misunderstanding: Purpose of stats testing is to falsify a Null hypothesis (Karl Popper)


### 1.2.1. Hypothesis are not models

Hypothesis are not models, therefore strict falsification impossible (weder hinreichend noch notwendig, logik!), Falsification is done to something other than the explanatory model (not always an opposite is possible, multi-criteria ptoblems)


### 1.2.2. Measurement matters

quality of method and data not absolute, Observation error (false positives, false negative), continuous hypotheses - not disproving/proving, but estimating

### 1.2.3 Falsification is consensual

but it is not logical

## 1.3. Tools for golem engineering

Instead of tests, we will have models as our tools

### 1.3.1. Bayesian data analysis

Pure logic, probabilities. Special case "Frequentist approach" - all probabibilities defined by events in large samples, parameters and models do not have probability distribution, only a "sampling distribution exist" 
Anyways, Bayesian statistics is logical and intuitive and that good.

### 1.3.2. Model comparison

Cross-Validation, Information Criteria will help with predictive accuracy, estimate of tendency of model to overfit data, spot influencial observations 

### 1.3.3. Multilevel models

uncertainty propagation happens, multilevel model help by using partial pooling (adjust estimates for repeat or imbalance in sampling, study variation, avoid averaging)

multilevel regression should always be norm over simple regression, but math hard

### 1.3.4. Graphical causal models

Cause and effect - wrong models may predict more accurately, but are still wrong!

Graphical Causal Model, Directed Acyclic Graph (DAG) will allow us to built a good statistical model, provided the causal model is true

## 1.4. Summary

math hard, statistics hard, logic good

Book parts
- 2,3 foundations Bayesian inference, basic tools, pure logic
- 4 to 9 multiple linear regression, causal inference when analyzing separate causal models for variable inclusion, plotting instead of interpreting, model complexity/overfitting, information theory and formal model comparison
- 9 to 12 linear models, markov chain monte carlo to fit models, maximum entropy, models, models, models
- 13 to 16 multi-level models, special models for error, missing data, spatial correlation, theoretical stats models

# Chapter 2: Small World and Large Worlds
Example: Small world is self-contained logical model, large world is context in which the model is deployed

## 2.1. The garden of forking data

Example Marbles

Counting possibilities: bag with 4 marbles black and white, so 5 possible combinations. If we draw blue, white, blue in a row with replacing, this event based on standard probability tree has different chance of happening for all 5 possible combinations. Three of the the combinations (all blue, all white) can not happen, the other have 3, 8 and 9 possible draws. 

Combining other info: Event draw 1 blue marbles, 0, 1,2,3,4 possible draws. We can multiply these two possible events

Probability:  instead of possible draws just multiply probability of marble combo 1/4 with possible options by it (e.g. 3,8,9). 
Plausibility of a specific combination of seeing event is proportional to ways to produce event X prior plausibility (!! order matters)


Definitions

*Parameter* p
*Likelihood* relative number of ways value p can produce data
*prior probability* prior plausibility of a specific p
*posterior probability* updated new probability 

posterior probability= prior probability*likelihood

## 2.2. Building a model 

Three steps (1) Create model based on narrative for data, descriptive or causal (2) Update by feeding data, called *Bayesian Updating* (3) Evaluate 


Bayesian updating may create plausibility curves (like pdf) based on observations, with each additional observation the curve changes 

! Bayesian estimates are valid at any sample size, but prior (initial plausability) has to be strong

## 2.3. Model Components

*Variables* - observed and unobserved, 

unobserved variables are called *Parameters*, each parameter has a prior plausibility *prior*, if prior is chosen based upon personal belief of analyst it is called *subjective bayesian*, prior needs to be chosen, evaluated and revised as part of model

observed variables have a distribution function, which is called *Likelihood* (term we will try to avoid), *binominal distribution* only two options (represents maximum entropy to count binary events)

nomenclature
X ~ Binominal (n,p) and p ~ Uniform (0,1)
X is observed variable, n is number of trials, p is unobserved parameter with a flat prior (0,1), can be other than uniform

## 2.4. Making the model go

*posterior distribution* update the model based on all variables etc 
Pr(p|(X,Y)) Probability of parameters, conditional on obsevred data

*Bayes Theorem* joint probability of data X,Y and particular value of p is Pr(X,Y,p)=Pr(X,Y|p)Pr(p)=Pr(p|X,Y)Pr(X,Y) also the other way around, with Pr(p) or Pr(X,Y) being prior probability, from that equation we will get 

Posterior=(Probability of Data x Prior)/(Average probability of Data)

Average probability of data (Pr(X,Y)) called *evidence* or *average likelihood* is literally average of probability over the prior

Pr(X,Y)+E(Pr(X,Y|p))=Integral Pr(X,Y|p)Pr(p)dp

E - *Expectation*, Averages are commonly called *Marginal*
Posterior is proportional to the product of prior and probability of the data

MOTOR of model is conditioning the prior on the data under rules of probability theory

BUT! math rules sometimes not enough -> numerical approximation techniques to the rescue (Grid approximation, quadratic approximation, Markov chain monte carlo MCMC)

*GRID APPROXIMATION* finite grid of parameter values p' calculate posterior probability for each grid call -> posterior distribution
R Code (1) Define grid (2) Computer prior value at each grid (3) compute likelihood at each grid (4) compute posterior at each grid (5)standardize posterior by dividing through sum of values


Example code, 20 grid cells, 6 events out of 9 binominal trials and the prior (plausibility of an event happening is 1 as one possible outcome when either or, seem slike prior is always 1 in binominal problems)
```{r}
# define grid
p_grid <- seq( from=0 , to=1 , length.out=20 )
# define prior
prior <- rep( 1 , 20 )
# compute likelihood at each value in grid
likelihood <- dbinom( 6 , size=9 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
plot(posterior)
```

*QUADRATIC APPROXIMATION* 
Region near peak of posterior will be nearly Gaussian/quadratic - described by mean and variance, low computational needs, often very exact

Two step R procedure (1) find posterior mode, usually by optimization algorithm trying to find peak (2) estimating posterior curvature near top to find peak, usually numerical 

Toss example with quap function from rethinking package
```{r}
library(rethinking)
globe.qa <- quap(
alist(
X ~ dbinom( X+Y ,p) , # binomial likelihood
p ~ dunif(0,1) # uniform prior
) ,
data=list(X=6,Y=3) )
# display summary of quadratic approximation
precis( globe.qa )
```
We provide formula, and list of data, quap does the rest behind the scenes. Results are posterior mean value p, curvature is standard deviation, assuming the posterior is gaussian. 

Code below is same thing but analytically and then compared to the posterior we already now (comparing the two curves)
```{r}
# analytical calculation
X <- 6
Y <- 3
curve( dbeta( x , X+1 , Y+1 ) , from=0 , to=1 )
# quadratic approximation
curve( dnorm( x , 0.67 , 0.16 ) , lty=2 , add=TRUE)

```

Quadratic approximation is often equivalent with maximum likelihood estimate (MLE) and its standard error. 

*MCMC* 
For (multi-level) models with many parameters grid and quadratic approx may have too much computational needs. -> MCMC as a counterintuitive model fitting technique. 

MCMC draws samples from posterior distribution directly, result collection of parameter values which frequencies correspond to posterior plausibilities. -> posterior from histogram of samples 


Random example (without explanation) *Metropolis Algorithm*
```{r}
n_samples <- 1000
p <- rep( NA , n_samples )
p[1] <- 0.5
W <- 6
L <- 3
for ( i in 2:n_samples ) {
p_new <- rnorm( 1 , p[i-1] , 0.1 )
if ( p_new < 0 ) p_new <- abs( p_new )
if ( p_new > 1 ) p_new <- 2 - p_new
q0 <- dbinom( W , W+L , p[i-1] )
q1 <- dbinom( W , W+L , p_new )
p[i] <- ifelse( runif(1) < q1/q0 , p_new , p[i-1] )
}
#compare to analytical posterior
dens( p , xlim=c(0,1) )
curve( dbeta( x , W+1 , L+1 ) , lty=2 , add=TRUE )
```


## 2.5. Summary
The target
of inference in Bayesian inference is a posterior probability distribution. Posterior probabilities state the relative numbers of ways each conjectured cause of the data could have produced the data. These relative numbers indicate plausibilities of the different conjectures.
These plausibilities are updated in light of observations, a process known as Bayesian updating.
More mechanically, a Bayesian model is a composite of variables and distributional definitions for these variables. The probability of the data, often called the likelihood, provides
the plausibility of an observation (data), given a fixed value for the parameters. The prior
provides the plausibility of each possible value of the parameters, before accounting for the
data. The rules of probability tell us that the logical way to compute the plausibilities, after
accounting for the data, is to use Bayes’ theorem. This results in the posterior distribution.
In practice, Bayesian models are fit to data using numerical techniques, like grid approximation, quadratic approximation, and Markov chain Monte Carlo. 



#Class Problems
```{r}
library(tidyverse)
world<-tibble(
  toss=c('L',"W","W","W","L","W","W","L","W","W","W","L")
)
table(world$toss)


#Gridapproximation
# define grid
p_grid <- seq( from=0 , to=1 , length.out=20 )
# define prior
prior <- rep( 1 , 20 )
# compute likelihood at each value in grid
likelihood <- dbinom( 8 , size=12 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
plot(posterior)

# analytical calculation
W <- 8
L <- 4
curve( dbeta( x , W+1 , L+1 ) , from=0 , to=1 )
```

# Chapter 3: Sampling the Imaginary

Vampire Examples
```{r}
Pr_Positive_Vampire <- 0.95 # Pr(positive test|Vampire) accuracy of correct detecting
 Pr_Positive_Mortal <- 0.01 # Pr(positive test|Mortal) false positives
 Pr_Vampire <- 0.001 # Pr(Vampire) percentage of population being vampires
 
 #Probability of a person being a vampire if they test positive
 Pr_Positive <- Pr_Positive_Vampire * Pr_Vampire +
 Pr_Positive_Mortal * ( 1- Pr_Vampire )
 ( Pr_Vampire_Positive <- Pr_Positive_Vampire*Pr_Vampire / Pr_Positive )
```
If condition of interest is very rare, false positives may be more frequent than true positives. 

Alternative thinking: 
 (1) In a population of 100,000 people, 100 of them are vampires.
 (2) Of the 100 who are vampires, 95 of them will test positive for vampirism.
 (3) Of the 99,900 mortals, 999 of them will test positive for vampirism.

Calculation Pr(vamp|positive)=95/(95+99)=95/1094=0.087

- in this chapter: use samples to summarize and simulate model output

## 3.1. Sampling for a grid-approximate posterior 

Generate posterior (e.g. globe tossing with 0.7/0.3 probabilities water/land with posterior meaning probability of p conditional on data), draw samples from posterior, plot samples, use rethinking package for the density estimate 

```{r}
 p_grid<-seq(from=0 , to=1,length.out=1000 )
 prob_p<-rep(1 ,1000 )
 prob_data<- dbinom( 6,size=9,prob=p_grid)
 posterior<- prob_data*prob_p
 posterior<- posterior/sum(posterior)

  samples<- sample(p_grid,prob=posterior , size=1e4,replace=TRUE)
  
  plot(samples)
  
 library(rethinking)
 dens(samples )
```

## 3.2. Sampling to summarize

After model produces posterior distribution, I need to summarize and interpret

*INTERVALS OF DEFINED BOUNDARIES*

Posterior probability of something being lower/higher/exactly a specific parameter value
Solution:Add up all probabilities of that condition

-> e.g. posterior probability of water being less than 0.5 using grid approximate posterior

```{r}
sum(posterior[p_grid<0.5])
```
-> alternative solution using samples directly (all samples less than 0.5 posterior)
```{r}
sum(samples<0.5)/1e4
```

Other things to play around with
```{r}
sum (samples=0.5)/1e4
sum (samples>0.5&samples<0.75)/1e4
```

*INTERVALS OF DEFINED MASS*

AKA *confidence intervals*, or if interval of posterior probability may call *credible interval*, but we call it *compatibility interval* because we check if a chosen range of parameter values is compatible with the model and data


```{r}
#new example, 3 waters in 3 tosses observed, with uniform (flat)  prior
 p_grid <- seq( from=0 , to=1 , length.out=1000 )
 prior <- rep(1,1000)
 likelihood <- dbinom( 3 , size=3 , prob=p_grid )
 posterior <- likelihood * prior
 posterior <- posterior / sum(posterior)
 samples <- sample( p_grid , size=1e4 , replace=TRUE , prob=posterior )
 
quantile(samples, 0.8) #boundaries of lower 80%
quantile(samples, c(0.1,0.9)) #middle 80% as Percentile Interval
HPDI(samples,prob=0.8) #narrowest interval containing 80%
```
Intervals with equal probability to each tail are called *percentile interval (PI)*, but better is *highest posterior density interval (HPDI)*, which is narrowest interval containing specific probability mass

HDPI
-> very similar to PI
-> advantage: if posterior is highly skewed
-> disadvantage: computational intensive, suffers from simulation variance (sensitive to number of samples), not established with scientist, hard to understand

Anyways: If choice of intervals makes a difference, we should not use intervals in the first place

*POINT ESTIMATES*
When we want to report a single value 

-> value with higher posterior, *maximum a posterior (MAP)* in this case values of p 
```{r}
p_grid[which.max(posterior)] #probability value with maximum posterior
chainmode(samples, adj=0.01) # with samples from posterior
mean(samples)
median(samples)
```

*Loss function* tells us cost with using a point estimate, different loss function imply different point estimates
```{r}
sum(posterior*abs(0.5-p_grid))
loss <- sapply(p_grid, function(d) sum(posterior*abs(d-p_grid))) #list of loss value for each possible decision 
p_grid[which.min(loss)] #this is actually the posterior median
```

## 3.3. Sampling to simulate prediction

Samples ease simulation of models observations. *Model design, model checking, software validation, research design, forecasting*

*DUMMY DATA*
Globe tossing example Pr(W|N,p)=N!/(W!(N-W!)!)*p^W*(1-p)^(N-W)

```{r}
dbinom(0:2, size=2, prob=0.7)
```
With probability of water 0.7, there is 9% chance of w=0, 42% of w=1 and 49% chance of w=2, when tossing 2 times
```{r}
dummy_w<-rbinom( 1e5, size=2 , prob=0.7 ) #generating 1e5 dummy observations (0,1, or 2 waters)
table(dummy_w)/1e5
dummy_l<-rbinom(1e5,size=9, prob=0.3)
simplehist(dummy_l, xlab="dummy land count")
```

*MODEL CHECKING*
means to ensure model fitting worked correctly and evaluating adequacy of model for purposes

-> software works? check correspondence between predictions and data used, no perfect way to check
-> model adequacy? observation uncertainty and uncertainty of prior, propagating parameter uncertainty!
averaging over density of posterior for p, *posterior predictive distribution*

Example 
```{r}
 w <- rbinom( 1e4 , size=9 , prob=0.6 )
simplehist(w)

#now replace value of 0.6 with samples from posterior
 w <- rbinom( 1e4 , size=9 , prob=samples)
 simplehist(w)
```

## 3.4. Summary

Samples of posterior turn integral calculus in data summary problems, produce intervals, point estimates, posterior predictive checks and other simulations 
Posterior predictive checks combine uncertainty about parameters with uncertainty about outcomes, as described by the assumed likelihood function